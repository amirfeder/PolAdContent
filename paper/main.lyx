#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding iso8859-15
\fontencoding T1
\font_roman "lmodern" "default"
\font_sans "default" "default"
\font_typewriter "lmodern" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing double
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 2
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.54cm
\topmargin 2.54cm
\rightmargin 2.54cm
\bottommargin 2.54cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Data Notes for 
\begin_inset Quotes eld
\end_inset

Text IV
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Section
Data
\end_layout

\begin_layout Subsection
Channels and Ratings
\end_layout

\begin_layout Standard
The ratings and channel position data come from Nielsen.
 These data are also at the zipcode level.
 They are the ratings (viewership) for Fox, CNN, and MSNBC.
 They also have the channel position of each network in that zipcode.
 
\end_layout

\begin_layout Subsection
News Transcripts
\end_layout

\begin_layout Standard
The text data includes the transcripts of all Prime Time shows for the three
 cable news networks, for the years 2001 through 2013.
 These were downloaded from Lexis.
\end_layout

\begin_layout Subsection
Republican Vote Share
\end_layout

\begin_layout Standard
The first outcome data is Republican vote share in the 2008 presidential
 election, constructed at the zipcode level by Martin and Yurukoglu (2017).
 They use the Harvard Election database to geolocate election precincts
 to zipcodes.
 We have data on 11376 zipcodes.
 
\end_layout

\begin_layout Subsection
Gallup Most Important Problem
\end_layout

\begin_layout Standard
The time-varying outcome is the monthly Most Important Problem question
 from Gallup.
 Each month, for 2001 through 2019, we have answers from 1000 respondents.
 It includes data on their 
\end_layout

\begin_layout Section
Text Methods
\end_layout

\begin_layout Subsection
Text Features
\end_layout

\begin_layout Standard
The first step is to featurize the transcripts to transform them to data.
 We first tokenized the transcripts and removed extra non-speech content.
 A parts-of-speech tagger was used to exclude all words that are not nouns,
 verbs, and adjectives.
 Using these filtered tokens, wethen counted unigrams, bigrams, and trigrams
 for each network.
\end_layout

\begin_layout Standard
To filter the feature set, we kept the 20,000 n-grams with the highest term
 frequency in the corpus.
 We then excluded any n-grams that did not appear in all three cable channels
 at least once.
\end_layout

\begin_layout Subsection
Narrative Generator
\end_layout

\begin_layout Standard
For each month and each network, we fine-tune a GPT-2 text generator.
 We then generate paragraphs of text as follows.
 The seed text is 
\begin_inset Quotes eld
\end_inset

The most important problem facing the country is [issue, e.g.
 gun control].
 The reasons are:
\begin_inset Quotes erd
\end_inset

 We then generate about 5 sentences.
 This text is used for presenting to the mechanical turkers.
\end_layout

\begin_layout Section
Weighted feature matrices
\end_layout

\begin_layout Subsection
Vote share analysis
\end_layout

\begin_layout Standard
Our data is zip code 
\begin_inset Formula $i$
\end_inset

, text feature 
\begin_inset Formula $j$
\end_inset

.
 To capture the exposure to each text feature, we start by constructing
 a panel of feature frequencies weighted by the normalized channel viewership
 (
\begin_inset Formula $v_{i,fox}+v_{i,cnn}+v_{i,msnbc}=1$
\end_inset

, that is, the share for channel 
\begin_inset Formula $c$
\end_inset

 of the total viewership for all news channels):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{ij}=v_{i,fox}x_{j,fox}+v_{i,cnn}x_{j,cnn}+v_{i,msnbc}x_{j,msnbc}
\end{equation}

\end_inset

This will serve as the endogenous regressor or treatment variable.
\end_layout

\begin_layout Standard
For the instruments 
\begin_inset Formula $z_{ij}$
\end_inset

, we use the channel positions.
 We also construct the position-weighted term frequencies, such that 
\begin_inset Formula $p_{i,c}$
\end_inset

 for channel 
\begin_inset Formula $c$
\end_inset

 is divided by the max channel position among the networks.
 So if Fox has the highest number at 50, 
\begin_inset Formula $p_{i,fox}=1$
\end_inset

, and if CNN is at 25, then 
\begin_inset Formula $p_{i,cnn}=.5$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
z_{ij}=p_{i,fox}x_{j,fox}+p_{i,cnn}x_{j,cnn}+p_{i,msnbc}x_{j,msnbc}
\end{equation}

\end_inset

relevance and randomness in exposure comes from the variation in text across
 cable networks, plus the fact that when there is a lower channel number
 people watch that channel more.
 
\end_layout

\begin_layout Subsection
Monthly gallup analysis
\end_layout

\begin_layout Standard
The data is county or zip code 
\begin_inset Formula $i$
\end_inset

, narrative 
\begin_inset Formula $j$
\end_inset

, on topic 
\begin_inset Formula $k$
\end_inset

, network 
\begin_inset Formula $c$
\end_inset

, at time 
\begin_inset Formula $t$
\end_inset

.
 Let 
\begin_inset Formula $x_{jkct}$
\end_inset

 be the frequency of narrative 
\begin_inset Formula $jk$
\end_inset

 on network 
\begin_inset Formula $c$
\end_inset

 during month 
\begin_inset Formula $t$
\end_inset

.
 Let 
\begin_inset Formula $v_{ic}$
\end_inset

 be viewership (or channel position) on network 
\begin_inset Formula $c$
\end_inset

 in locality 
\begin_inset Formula $i$
\end_inset

.
 Local exposure to narrative 
\begin_inset Formula $jk$
\end_inset

 is 
\begin_inset Formula 
\[
x_{ijkct}=\sum_{c}v_{ic}x_{jkct}
\]

\end_inset

We apply deep IV to this set of high-dimensional treatments to estimate
\begin_inset Formula 
\[
y_{ikt}=f(x_{jkct})
\]

\end_inset

where 
\begin_inset Formula $y_{ikt}$
\end_inset

 is the proportion of people in locality 
\begin_inset Formula $i$
\end_inset

 at month 
\begin_inset Formula $t$
\end_inset

 who name topic 
\begin_inset Formula $k$
\end_inset

 as the most important problem.
\end_layout

\begin_layout Standard
We then learn a set of causal feature importances for each narrative 
\begin_inset Formula $j$
\end_inset

.
 Those that most contribute to an increase or decrease in 
\begin_inset Formula $\hat{y}_{ikt}$
\end_inset

 are then used in the mechanical turk study.
\end_layout

\end_body
\end_document
